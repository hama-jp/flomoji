import { createNodeDefinition } from './types';
import type { WorkflowNode, NodeInputs, INodeExecutionContext, NodeOutput } from '../../types';
import type { LLMNodeData } from '../../types/nodeData';
import llmService from '../../services/llmService';

// Consoleæ‹¡å¼µç”¨ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«å‹å®šç¾©
declare global {
  interface Console {
    lastCall?: string;
  }
}

/**
 * LLMç”Ÿæˆãƒãƒ¼ãƒ‰ã®å®Ÿè¡Œå‡¦ç†
 * @param {Object} node - ãƒãƒ¼ãƒ‰ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
 * @param {Object} inputs - å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
 * @param {Object} context - å®Ÿè¡Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
 * @returns {Promise<string>} LLMã‹ã‚‰ã®å¿œç­”
 */
async function executeLLMNode(node: WorkflowNode, inputs: NodeInputs, context?: INodeExecutionContext): Promise<NodeOutput> {
  const nodeData = node.data as LLMNodeData;
  const temperature = nodeData.temperature || 0.7;
  const model = nodeData.model;
  const provider = (nodeData.provider || 'openai') as 'openai' | 'anthropic' | 'local' | 'custom';
  const systemPrompt = nodeData.systemPrompt || null;
  const maxTokens = nodeData.maxTokens || null;
  
  // å…¥åŠ›ã‚’ãã®ã¾ã¾LLMã«é€ä¿¡
  const inputValues = Object.values(inputs).filter(v => v !== undefined && v !== null);
  if (inputValues.length === 0) {
    throw new Error('LLMãƒãƒ¼ãƒ‰ã«å…¥åŠ›ãŒã‚ã‚Šã¾ã›ã‚“');
  }
  
  const finalPrompt = inputValues.join('\n');
  
  try {
    // ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’ãƒ­ã‚°ã«è¨˜éŒ²
    context?.addLog('info', `LLMãƒãƒ¼ãƒ‰ã‚’å®Ÿè¡Œä¸­`, node.id, { 
      model,
      provider,
      temperature,
      hasSystemPrompt: !!systemPrompt,
      maxTokens: maxTokens || 'default'
    });
    
    const currentSettings = llmService.loadSettings();
    context?.addLog('debug', `ç¾åœ¨ã®è¨­å®š`, node.id, { currentSettings });
    
    const nodeSpecificOptions = {
      provider,
      model,
      temperature,
      apiKey: currentSettings.apiKey,
      baseUrl: currentSettings.baseUrl,
      maxTokens: maxTokens || currentSettings.maxTokens  // ãƒãƒ¼ãƒ‰è¨­å®šã‚’å„ªå…ˆã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§ã‚·ã‚¹ãƒ†ãƒ è¨­å®š
    };
    context?.addLog('debug', `ãƒãƒ¼ãƒ‰å›ºæœ‰ã‚ªãƒ—ã‚·ãƒ§ãƒ³`, node.id, { 
      nodeSpecificOptions,
      maxTokensSource: maxTokens ? 'node-specific' : 'system-default',
      nodeMaxTokens: maxTokens,
      systemMaxTokens: currentSettings.maxTokens,
      finalMaxTokens: maxTokens || currentSettings.maxTokens
    });
    
    context?.addLog('debug', `llmService.sendMessageå‘¼ã³å‡ºã—ä¸­`, node.id, { 
      finalPrompt, 
      systemPrompt, 
      nodeSpecificOptions 
    });
    
    // è©³ç´°ãªAPIãƒ¬ã‚¹ãƒãƒ³ã‚¹æƒ…å ±ã‚’å–å¾—ã™ã‚‹ãŸã‚ã®ãƒ†ãƒ³ãƒãƒ©ãƒªãƒ¼ãªä¿®æ­£
    const result = await llmService.sendMessage(finalPrompt, systemPrompt, nodeSpecificOptions);
    
    // ãƒ–ãƒ©ã‚¦ã‚¶ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã®æœ€æ–°ãƒ­ã‚°ï¼ˆAPIãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼‰ã‚’å–å¾—ã‚’è©¦ã¿ã‚‹
    setTimeout(() => {
      if (console.lastCall && console.lastCall.includes('LLM Response:')) {
        context?.addLog('debug', `ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã‹ã‚‰APIãƒ¬ã‚¹ãƒãƒ³ã‚¹æƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ - ç›´æ¥F12ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„`, node.id);
      }
    }, 100);
    
    context?.addLog('debug', `llmService.sendMessageå‘¼ã³å‡ºã—å®Œäº†`, node.id, { 
      resultType: typeof result, 
      resultLength: result?.length,
      result: result,
      isEmptyResponse: result === 'ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒç©ºã§ã™'
    });
    
    context?.addLog('info', `LLMã‹ã‚‰ã®å¿œç­”ã‚’å—ä¿¡ã—ã¾ã—ãŸ`, node.id, { response: result?.substring(0, 100) + '...' });
    
    return result;
  } catch (error: any) {
    context?.addLog('error', `LLMãƒãƒ¼ãƒ‰å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: ${error instanceof Error ? error.message : String(error)}`, node.id, { error: error instanceof Error ? error.stack : undefined });
    throw error;
  }
}

/**
 * LLMç”Ÿæˆãƒãƒ¼ãƒ‰ã®å®šç¾©
 * AIãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹
 */
export const LLMNode = createNodeDefinition(
  'LLM Generation',
  'ğŸ¤–',
  'blue',
  ['input'], // å…¥åŠ›ãƒãƒ¼ãƒˆ: input
  ['output'], // å‡ºåŠ›ãƒãƒ¼ãƒˆ: output
  {
    temperature: 1.0,
    model: 'gpt-5-nano',
    systemPrompt: '',
    maxTokens: 50000
  },
  executeLLMNode, // å®Ÿè¡Œãƒ¡ã‚½ãƒƒãƒ‰
  {
    description: 'Generate text using AI language models. Supports system prompts, temperature settings, and model selection.',
    category: 'ai'
  }
);

export default LLMNode;